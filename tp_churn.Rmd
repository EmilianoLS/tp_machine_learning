---
title: "TP_CHURN"
output:
  html_notebook: default
  pdf_document: default
  word_document: default
---
  Instalo las librerias y asigno la ruta en github donde esta la base a estudiar

```{r message=FALSE, warning=FALSE}
#  busco librerias a usar y asigno la ruta en github 
#rm(list = ls())

setwd('D:/DS_Projects/tp_machine_learning')
#setwd('C:/Users/elosasso/OneDrive - Universidad Torcuato Di Tella/Machine Learning/TP/script/tp_machine_learning')
#setwd('C:/Users/tutif/Desktop/emi/tp_machine_learning')
source('functions_R.R')
Ruta <- "https://raw.githubusercontent.com/EmilianoLS/tp_machine_learning/master/train.csv"


```

# Carga de datos

```{r message=FALSE}
data <- read_csv(url(Ruta))
data$TARGET <- factor(if_else(data$TARGET == 1, 'churn', 'no_churn'))
```

# Analisis preeliminar

Antes de hacer un análisis profundo, observamos la estructura general de los datos, cómo se encuentran dispuestos, cantidad de columnas, filas, tipos de datos, etc.

Esta información nos va a dar un entendimiento global de los datos, permitiendo descubrir insights que nos ayuden a descubrir alguna inconsistencia que haya que corregir, previo a efectuar un análisis descriptivo más detallado.


```{r}
# Veo como se estructuran los datos

head(data)
dim(data)       
```

El dataframe contiene 33008 registros y 312 variables. No hay mucha descripción sobre el contenido de cada una, con excepción de *age*,*nac*,*ID*.

Adicionalmente, en un archivo se indica la siguiente descripción sobre las variables:

* Variables con el prefijo *'imp'* y *'saldo'* indican importes y saldos, por lo que son variables numéricas.
* Variables con el prefijo *'num'* indican un cierto número de cosas: Contactos al call center, cantidad de productos contratados, etc.
* Variables con el prefijo *'ind'* indican un flag, por lo que son variables de tipo booleanas.
* Variables con el prefijo *'delta'* indican cambios en el tiempo de alguna de las variables

Luego existen tres variables *var21*, *var36* y *var38* que no se sabe bien qué describen por lo que se van a estudiar posteriormente. 

## Clases de datos en las variables

```{r}
# Clases de variables
table(sapply(data, class))   
```

Como se ve, no existen variables del tipo categoricas, son todas numericas excepto por la variable TARGET. 

Sin embargo, como se mencionó anteriormente, las variables con prefijo *'ind'* deberían ser booleanas.

Cambiamos estas variables a su valor correspondiente.

#### Conversión de variables

```{r}
# Creo un vector con las columnas para castear
bool_columns <- names(data)[grepl("ind", names(data))]
# Hago el cast
data[bool_columns] <- lapply(data[bool_columns], as.logical)
# Vuelvo a chequear las clases de variables

table(sapply(data, class)) 
```

Como se ve, ahora todas las variables tienen su clase correspondiente.

El siguiente paso es hacer un análisis exploratorio de las variables para entender cómo se relacionan, buscar si existen tendencias o alguna variable que permita diferenciar lo suficientemente bien a los grupos que hacen churn vs los que no lo hacen.

# Exploratory Data Analysis

Antes de empezar a analizar las muchas variables similares que hay, comenzamos estudiando las distribuciones para las variables de edad, la target y las tres variables desconocidas. 

#### Distribución de la variable *'Age'*

```{r}

histplot <- ggplot(data) + geom_histogram(aes(x = data$age), bins = 100) + 
  geom_vline(aes(xintercept = mean(data$age)), colour = 'red',linetype = "longdash") + 
  geom_vline(aes(xintercept = median(data$age)), colour = 'blue', linetype = 'longdash') + 
  annotate('text',x = mean(data$age)+10, y = 2000, label = 'Promedio', colour = 'red') +
  annotate('text',x = median(data$age)-10, y = 4000, label = 'Media', colour = 'blue') +
  ggtitle('Distribución de la edad') + xlab('Edad')

caja <- ggplot(data) + geom_boxplot(aes(x = age)) + ggtitle('Distribución de la edad') + xlab('Edad')
grid.arrange(histplot, caja, ncol = 1)

```


La edad no parece seguir una distribución muy normal, pareciera estar más cercana a los valores pequeños (alrededor de los 20-30 años), con mayor concentración en una edad en particular que parece que ocurre con mayor fecuencia:

```{r}
ages <- as.data.frame(table(data$age))
order.age <- order(ages$Freq, decreasing = TRUE)
ages <- ages[order.age, ]
ages[c(1:5),]
```

La edad más común es de 23 años, incluso el top 5 de edades va de los 23 a los 27. 

Existen algunos datos atípicos, extremos, que están por debajo de los 20 años. Como la empresa se encuentra anonimizada, no conocemos el negocio, por lo que no sabemos si alguna edad relativamente pequeña, como por ejemplo 15 años, es algo raro o no. Por ejemplo, en el caso de que se traten de clientes de un banco, sería raro que una persona de menos de 18 años sea cliente, pero si se trata de datos de una aplicación móvil, no necesariamente es algo fuera de lo común.

```{r}
cat('Edad mínima:', min(data$age), '\nEdad máxima:', max(data$age))
```

Evidentemente existen valores atípicos en al menos las edades más bajas, ya que un cliente de 5 años, no importa el negocio, es un outlier.

#### Distribución de la edad por grupo de churn/no churn

```{r}
data %>% ggplot(aes(x = age, fill = TARGET)) + geom_histogram(color="#e9ecef", alpha=0.4, position = 'identity', bins = 50) + ggtitle('Distribución de la edad según el target')
```

#### Proporción de usuarios

```{r}
ggplot(data, aes(TARGET)) + geom_bar() + ggtitle('Proporción de churn/no churn')
```

Si miramos la proporción:

```{r}
prop.table(table(data$TARGET))
```

Evidentemente la variable se encuentra desbalanceada, con un 90% de los usuarios que no hicieron churn, contra un 10% de usuarios que si lo hicieron. 

Sin embargo, no es un desbalanceo tal que implique (por el momento) tomar alguna medida de rebalanceo como *undersampling* u *oversampling*.

# Análisis de variables desconocidas

Como dijimos antes, hay algunas variables que no se dicen mucho sobre lo que significan. Por eso analizamos la distribución de las mismas, para ver si así entendemos mejor qué tipo de dato es y lo que pretende describir.

```{r,fig.width = 10, fig.height = 7}
plot1 <- data %>% ggplot(aes(x = var21, fill = TARGET)) + geom_histogram(color="#e9ecef", alpha=0.4, position = 'identity', bins = 50) + ggtitle('Distribución de la var21 según el target')
plot2 <- data %>% ggplot(aes(x = var36, fill = TARGET)) + geom_histogram(color="#e9ecef", alpha=0.4, position = 'identity', bins = 50) + ggtitle('Distribución de la var36 según el target')
plot3 <- data %>% ggplot(aes(x = var38, fill = TARGET)) + geom_histogram(color="#e9ecef", alpha=0.4, position = 'identity', bins = 50) + ggtitle('Distribución de la var38 según el target')

ggarrange(plot1,plot2,plot3,ncol = 1)
```

```{r}
# Miro los quantiles para ver qué tanto varían los datos

quantile(data$var21)
quantile(data$var36)
quantile(data$var38)

# Var36 y 38 parecen mostrar datos más distribuidos, pero var21 pareciera solamente tener dos valores: 0 y 30000
```

```{r,fig.width = 10, fig.height = 7}
# Distribución de var21 para valores superiores a 0
data$temp <- if_else(data$var21 > 0, 'mayor_0','igual_0')
plot1 <- filter(data,var21 > 0) %>% ggplot(aes(x = var21, fill = TARGET)) + geom_histogram(color="#e9ecef", alpha=0.4, position = 'identity', bins = 50) + ggtitle('Distribución de la var21 según el target')
plot2 <- ggplot(data, aes(temp)) + geom_bar() + ggtitle('Proporción de var21 mayor a 0')

ggarrange(plot1,plot2, ncol = 1)
data$temp <- NULL
```

Vemos que la proporción de datos para la variable var21 con valor mayor a 0 es muy baja (aproximadamente un 1%). Pensabamos que tal vez esta variable solo tomaba dos valores posibles, pero vemos que existen algunos valores por encima de 0 distintos de 30000 aunque no son muchos.

Notamos también que en líneas generales ninguna de las variables separa muy bien los grupos. 
También son variables que toman valores muy altos, muy extremos. Por ejemplo para el caso de la var21, casi que pueden considerarse como *outliers*.


# Data Cleaning

En esta etapa se toman ciertas medidas 'sanitarias' para eliminar valores atípicos, como los que se encontraron en la variable *'age'* y se hace un tratamiento sobre los valores nulos.

### Valores Nulos o *NAs*

```{r}
# Busco todos los nulos
sum(is.na(data))
```

56 es una cantidad relativamente baja de *NAs*, sobretodo teniendo en cuenta que hay aproximadamente 30.000 registros. Vemos en dónde se encuentran estos registros faltantes:

```{r message=FALSE, warning=FALSE}
# Llamo a una función para encontrar la proporción nulos por columna

find_nulls(data)
```

El resultado anterior ilustra que solamente una columna es la que tiene el total de valores *NAs* y es la variable *'nac'*, con un 16% de sus registros como valores missings. Existen distintas formas para tratar a estos valores:

* Eliminar los registros que tengan valores nulos
* Eliminar toda la variable con nulos
* Imputar los valores faltantes por algún valor que creamos correspondiente (promedio, valor más frecuente, etc.)

Debido a la baja cantidad de nulos que representan, y como tampoco tenemos del todo claro qué representa la variable, optamos por eliminar estos registros.

```{r}
# Eliminamos los nulos
data <- drop_na(data)
dim(data)
```

Arriba se encuentran las nuevas dimensiones del dataset. Lógicamente, el resultado es el mismo en cuanto a columnas, mientras que en cuanto a registros, disminuyó en 56, esto corresponde a los valores que acabamos de retirar.

Con esto, cubrimos todo lo que podía hacerse respecto de los valores *NAs* ya que no hay más columnas con éstos. 


# Modelado

Habiendo analizado los datos y luego de hacer algunas limpiezas "higiénicas", procedemos a la etapa de modelado.
Se procederá a aplicar una serie de técnicas y modelos para:

* Obtener una predicción aceptable que sirva para determinar con un cierto grado de certeza la probabilidad de que un usuario/ cliente haga churn en el futuro
* Seleccionar variables más importantes para el modelo
* Determinar las métricas de performance más adecuadas

Para poder hacer esto, lo primero que debemos hacer es separar los datos en un conjunto de *entrenamiento* y otro de *validación*.
Esto se hace para poder simular datos **nuevos** sobre los que probar cualquier tipo de transformación que se haga sobre los datos. Básicamente lo que buscamos con esta separación es mantener un conjunto de datos sobre el cual haremos modificaciones y transformaciones o creando nuevas variables y con el cual se van a entrenar los modelos y otro conjunto para testear que todo lo hecho efectivamente ayude a mejorar la predicción.

```{r}
# Seteo una semilla para recrear las distintas pruebas
set.seed(999)    
# Selecciono los indices de entrenamiento
inTraining <- createDataPartition(data$TARGET, p = .75, list = FALSE)

# Separo los dos conjuntos de datos
train_data <- data[inTraining, ]
validation_data <- data[-inTraining, ]         

# Verifico que ambos conjuntos estén balanceados
print('Distribucion del target en el conjunto de entrenamiento')
prop.table(table(train_data$TARGET))
print('Distribucion del target en el conjunto de validacion')
prop.table(table(validation_data$TARGET))
```

Con ambos conjuntos, procedemos a realizar los modelados, manteniendo siempre el conjunto de validación sin ninguna alteración, es decir, solamente lo consultamos para verificar eficiencia del modelo, no para entrenar ni ninguna otra operación.

### Modelo Benchmark

Lo primero que decidimos hacer es tener una referencia de lo que podemos llegar a esperar con los datos en el estado más crudo, sin ningún tipo de tratamiento ni optimización. Por eso elegimos correr un modelo sencillo como un árbol de decisión para ver cómo performa. Esta será nuestra medida de comparación y sobre la cuál iremos construyendo hasta alcanzar mejores resultados. Luego haremos unos modelos más, aplicando ciertas transformaciones:

* Transformación logarítmica
* Normalización
* Ambas anteriores

Todas las pruebas se evaluarán sobre el conjunto de validación luego de entrenarse los modelos, para verificar si alguna de ellas mejora la performance sobre el modelo benchmark.
El motivo para tales transformaciones, especialmente la logarítmica, es porque muchas de las variables se encuentran muy concentradas en valores pequeños mientras que los valores más grandes tienen pocos registros. Esto significa que las variables no siguen una distribución normal, y una transfomación logarítmica puede ayudar a normalizar la distribución.

Si bien el modelo de árboles es bastante robusto frente a distribuciones de este estilo, al igual que a dataframes no normalizados, vale la pena intentar hacer las transformaciones.


```{r}
# Entrenamos el modelo Arbol
######################################  MODELO BENCHMARK  ######################################

fitControl <- trainControl(method = 'cv', 
                           number = 5,
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary)

rpartFit <- train(TARGET ~ .,data = train_data, 
                  method = 'rpart', 
                  trControl = fitControl,
                  metric = 'ROC')

train_bechmark_result <- max(rpartFit$results$ROC)                            # Guardo resultados del entrenamiento

# Evaluamos en Test

pred <- predict(rpartFit,validation_data, type = 'prob')
pred$pred <- factor(if_else(pred$churn >= .5, 'churn','no_churn'))
pred$obs <- validation_data$TARGET

test_results_benchmark <- twoClassSummary(pred, lev = levels(pred$obs))
test_results_benchmark_roc <- as.numeric(test_results_benchmark[1])         # Guardo resultados de testing

###########################################  MODELO LOG TRF  ######################################

# Transformación logarítmica de los datos


train_data_log <- log_function(train_data, 'TARGET')      # Función definida que aplica la transformación logarítmica sobre los datos numéricos
validation_data_log <- log_function(validation_data, 'TARGET')


# Entrenamos el modelo

rpartFit_log <- train(TARGET ~ .,data = train_data_log, 
                  method = 'rpart', 
                  trControl = fitControl,
                  metric = 'ROC')
train_logtf_result <- max(rpartFit_log$results$ROC)

# Evaluamos en Test

pred <- predict(rpartFit_log,validation_data_log, type = 'prob')
pred$pred <- factor(if_else(pred$churn >= .5, 'churn','no_churn'))
pred$obs <- validation_data_log$TARGET

test_results_log <- twoClassSummary(pred, lev = levels(pred$obs))
test_results_log_roc <- as.numeric(test_results_log[1])

###########################################  MODELO NORMALIZADO  ######################################

# Normalizamos los datos con la función preProcess

preprocess_norm <- preProcess(train_data, norm = c('center','scale'))

# Aplicamos el modelo aprendido al conjunto de entrenamiento y validación
# Guardamos en otra variable para no perder la original

train_data_norm <- predict(preprocess_norm, train_data)
validation_data_norm <- predict(preprocess_norm, validation_data)

# Entrenamos el modelo

rpartFit_norm <- train(TARGET ~ .,data = train_data_norm, 
                  method = 'rpart', 
                  trControl = fitControl,
                  metric = 'ROC')
train_norm_result <- max(rpartFit_norm$results$ROC)

# Evaluamos en Test

pred <- predict(rpartFit_norm,validation_data_norm, type = 'prob')
pred$pred <- factor(if_else(pred$churn >= .5, 'churn','no_churn'))
pred$obs <- validation_data_norm$TARGET

test_results_norm <- twoClassSummary(pred, lev = levels(pred$obs))
test_results_norm_roc <- as.numeric(test_results_norm[1])

######################################  MODELO NORMALIZADO + LOG TRF  ######################################

# Aplicamos una transformación log y normalizamos los datos

preprocess_norm_log <- preProcess(train_data_log, norm = c('center','scale'))

# Aplico el modelo aprendido sobre los conjuntos de entrenamiento y validación

train_data_log_norm <- predict(preprocess_norm_log, train_data_log)
validation_data_log_norm <- predict(preprocess_norm_log, validation_data_log)

# Entrenamos el modelo

rpartFit_log_norm <- train(TARGET ~ .,data = train_data_log_norm, 
                  method = 'rpart', 
                  trControl = fitControl,
                  metric = 'ROC')
train_log_norm_result <- max(rpartFit_log_norm$results$ROC)

# Evaluamos en Test

pred <- predict(rpartFit_log_norm,validation_data_log_norm, type = 'prob')
pred$pred <- factor(if_else(pred$churn >= .5, 'churn','no_churn'))
pred$obs <- validation_data_log_norm$TARGET

test_results_log_norm <- twoClassSummary(pred, lev = levels(pred$obs))
test_results_log_norm_roc <- as.numeric(test_results_log_norm[1])

######################################  TABLA FINAL CON LOS RESULTADOS  ######################################

modelos <- c('Benchmark','Transformación log','Normalizado','Ambas transformaciones')
resultados_training <- c(train_bechmark_result,train_logtf_result,train_norm_result,train_log_norm_result)
resultados_validation <- c(test_results_benchmark_roc,test_results_log_roc,test_results_norm_roc,test_results_log_norm_roc)
data.frame(modelos,resultados_training,resultados_validation)
```



```{r, fig.width = 10, fig.height = 7}

# Graficamos todas las curvas ROC
# Modelo Benchmark

pred <- predict(rpartFit,validation_data, type = 'prob')[,2]
pred2 <- prediction(pred, validation_data$TARGET)
perf <- performance(pred2,"tpr","fpr")

# Modelo log trf

pred_log <- predict(rpartFit_log,validation_data_log, type = 'prob')[,2]
pred2_log <- prediction(pred_log, validation_data_log$TARGET)
perf_log <- performance(pred2_log,"tpr","fpr")

# Modelo normalizado

pred_norm <- predict(rpartFit_norm,validation_data_norm, type = 'prob')[,2]
pred2_norm <- prediction(pred_norm, validation_data_norm$TARGET)
perf_norm <- performance(pred2_norm,"tpr","fpr")

# Modelo mixeado

pred_log_norm <- predict(rpartFit_log_norm,validation_data_log_norm, type = 'prob')[,2]
pred2_log_norm <- prediction(pred_log_norm, validation_data_log_norm$TARGET)
perf_log_norm <- performance(pred2_log_norm,"tpr","fpr")

# Graficamos la curva ROC

par(mfrow = c(2,2))

plot(perf, main="Curva ROC - Modelo Benchmark", colorize=T)
plot(perf_log, main="Curva ROC - Modelo log", colorize=T)
plot(perf_norm, main="Curva ROC - Modelo normalizado", colorize=T)
plot(perf_log_norm, main="Curva ROC - Modelo mix", colorize=T)
```

Vemos la importancia de las variables:

```{r, fig.width = 10, fig.height = 7}
# Open a pdf file
pdf("var_importance_rpart.pdf") 
par(mfrow = c(2,2))

plot(varImp(rpartFit), top = 30, main = 'Feature Importance - Modelo Benchmark')
plot(varImp(rpartFit_log), top = 30, main = 'Feature Importance - Modelo Log')
plot(varImp(rpartFit_norm), top = 30, main = 'Feature Importance - Modelo Normalizado')
plot(varImp(rpartFit_log_norm), top = 30, main = 'Feature Importance - Modelo Mix')
dev.off()
```

Las princpales conclusiones que podemos sacar son:

* No hubo ningún modelo que superara el valor de AUC del modelo benchmark, al menos no significativamente (siempre evaluando sobre la performance en validación)
* Podemos decir que el modelo, en su forma más básica tiene una AUC básica de 0.7, este no es un mal resultado en sí mismo, teniendo en cuenta que todavía no se hizo ningún tipo de ingeniería de atributos.
* Observamos que los valores de entrenamiento aumentan respecto al modelo benchmark, especialmente para el modelo *log* y *norm*. Sin embargo, baja bastante la performance en validación, señalando un claro caso de *overfitting*. Uno de los motivos por lo cual esto puede estar pasando es debido al alto número de dimensiones (variables), además de que estos modelos consisten en un solo árbol.
* Si bien el orden varía, todos los modelos mantienen las mismas variables "relevantes" según se observa en los gráficos de importancia de variables.

Es necesario aclarar que optamos por tomar como medida de performance el *área bajo la curva ROC o AUC*. Esto es así ya que es una medida que, a diferencia de otras como el *accuracy* o *recall* o la *matriz de confusión*, es independiente del punto de corte que se elija. Con esto nos referimos a que no toma en consideración un valor umbral determinado para decidir si un elemento pertenece a una clase o a otra. Esto es deseable ya que nos permite entender de forma más genérica la forma en la que está performando el modelo.

El principal objetivo es reducir la dimensionalidad del dataframe. Para esto combinaremos una serie de modelos que determinen la importancia de los atributos, y utilizaremos la combinación de esos resultados para determinar cuáles son las variables más importantes.

# Selección de atributos

Habiendo evaluado algunos modelos, vimos que no hay ninguno significativamente superior al benchmark, sin embargo, decidimos mantener el modelo normalizado, ya que mantiene la misma performance en validación que el modelo base, y tiene la ventaja de que contamos con los datos normalizado, cosa necesaria para algunos modelos que pasaremos a evaluar.

Para decidir qué atributos mantener y cuáles descartar, decidimos basarnos en una serie de modelos y herramientas que nos determinen la importancia de cada variable al momento de modelizar, y las correlaciones y variabilidades que hay en la variables predictoras.
Combinando estos métodos, evaluando siempre contra el conjunto de validación, esperamos reducir la cantidad de atributos de forma considerable para poder usar ese "nuevo dataset" como input para modelos más avanzados (como por ejemplo SVM o NN).

La primer parte que haremos será correr dos o tres modelos que, además de darnos un cierto valor de AUC y nos permtirán ver si el modelo mejora respecto al benchmark, sino que también nos van a devolver una cierta importancia asignada a cada variable. 

## Feature importance

### Modelo log regularizado

```{r}
# Recordemos que la libreria glmnet consume matrices, no dataframes

#train_data$TARGET <- as.logical.factor(if_else(train_data_norm$TARGET == 'churn',1,0))
#validation_data$TARGET <- as.logical.factor(if_else(validation_data_norm$TARGET == 'churn',1,0))

matrix_train = model.matrix( ~ .-1, train_data)
matrix_test = model.matrix(~.-1, validation_data)

x_train <- as.matrix(matrix_train[,-313])
y_train = as.matrix(matrix_train[,313]) # 0/1 flag.

x_test <- as.matrix(matrix_test[,-313])
y_test<- as.matrix(matrix_test[,313])

############################################## MODELO DE REG LOG SIN REGULARIZACIÓN ###################################################

grid.l =exp(seq(1 , -10 , length = 20))
grid.l

cv.out = cv.glmnet(x_train, y_train, 
                   family = 'binomial', # Modelo logistico.
                   type.measure="auc", # Metrica del CV (no depende del umbral).
                   lambda = grid.l, 
                   alpha = 1, # LASSO.
                   nfolds = 5,
                   standardize = TRUE)

# Visualuzamos la evolucion de los valores de las variables 
# en funcion de los distintos lambda log(??)

plot (cv.out)
```

```{r}
# Guardamos el mejor lambda

bestlam = cv.out$lambda.min

# Predicciones sobre el conjunto de Train con el mejor lambda

pred_train = predict(cv.out, s = bestlam , newx = x_train, type = 'response')

# Predicciones en Test

pred_test = predict(cv.out, s = bestlam , newx = x_test, type = 'response')

# Calculo la curva ROC en Train

pred2_train <- prediction(pred_train, y_train)
perf_train <- performance(pred2_train,"tpr","fpr")

# Calculo la curva ROC en Test

pred2_test <- prediction(pred_test, y_test)
perf_test <- performance(pred2_test,"tpr","fpr")

# Graficamos la curva ROC

par(mfrow = c(1,2))

plot(perf_train, main = "Curva ROC training", colorize = T)
plot(perf_test, main = "Curva ROC testing", colorize = T)

# Calculamos el AUC en Train

auc_train <- performance(pred2_train, measure = "auc")
#auc_train@y.values[[1]] 

# Calculamos el AUC en Test

auc_test <- performance(pred2_test, measure = "auc")
#auc_test@y.values[[1]] 
```

```{r}
resultados_log_training <- c(auc_train@y.values[[1]] )
resultados_log_testing <- c(auc_test@y.values[[1]] )
modelo <- c('Regularización Lasso optimizada')

data.frame(modelo, Training = resultados_log_training, Validation = resultados_log_testing)
```
```{r}
# Guardamos las variables más importantes

coef_matrix <- as.matrix(coef(cv.out, cv.out$lambda.min))
important_features <- as.matrix(coef_matrix[coef_matrix != 0,])
important_features <- row.names(important_features)

important_features
```

```{r}
# Aplicamos un modelo de random Forest


control <- trainControl(method = 'cv', 
                        number = 5,
                        classProbs = TRUE,
                        summaryFunction = twoClassSummary,
                        verboseIter = TRUE)


set.seed(123)

#Number randomely variable selected is mtry

mtry <- sqrt(ncol(train_data))

#tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(TARGET~., 
                      data = train_data, 
                      method = 'rf', 
                      metric = 'ROC', 
                      #tuneGrid = tunegrid, 
                      trControl = control,
                      preProcess = c('center','scale'))
print(rf_default)
```
```{r}
# Guardo el mejor AUC de Train

rf_basic_results_training <- max(rf_default$results$ROC)

# Evaluamos en Test

pred <- predict(rf_default,validation_data, type = 'prob')
pred$pred <- factor(if_else(pred$churn >= .5, 'churn','no_churn'))
pred$obs <- validation_data$TARGET

test_results_rf_basic <- twoClassSummary(pred, lev = levels(pred$obs))
test_results_rf_basic_roc <- as.numeric(test_results_rf_basic[1])

#################################################### MODELO RANDOM FOREST OPTIMIZADO ###########################################

control_cv <- trainControl(method='repeatedcv', 
                        number=5, 
                        repeats=3,
                        classProbs = TRUE,
                        search = 'random',
                        summaryFunction = twoClassSummary,
                        verboseIter = TRUE)

# Realizo una busqueda aleatoria del parametro mtry mientras coloco distintos valores
# de profundidad del árbol

modellist <- list()

#train with different ntree parameters
for (ntree in c(1000,1500,2000,2500)){
  
  set.seed(123)
  
  rf_optimized <- train(TARGET ~ .,
               data = train_data,
               method = 'rf',
               metric = 'ROC',
               tuneLength  = 15, 
               trControl = control_cv,
               preProcess = c('center','scale'),
               ntree = ntree)
  
  key <- toString(ntree)                          # Guardo el numero de arboles usados
  best_mtry <- as.numeric(rf_optimized$bestTune)  # Me guardo la mejor mtry de la corrida
  
  modellist[[key]] <- c(max(rf_optimized$results$ROC), best_mtry)
}


```

```{r}
#Compare results

results <- resamples(modellist)
summary(results)

# Guardo el mejor AUC de Train

rf_optimized_results_training <- max(rf_optimized$results$ROC)

# Evaluamos en Test

pred <- predict(rf_optimized,validation_data, type = 'prob')
pred$pred <- factor(if_else(pred$churn >= .5, 'churn','no_churn'))
pred$obs <- validation_data$TARGET

test_results_rf_optimized <- twoClassSummary(pred, lev = levels(pred$obs))
test_results_rf_optimized_roc <- as.numeric(test_results_rf_optimized[1])
```

Antes de avanzar con la selección de atributos, notamos que al momento de normalizar se advirtió que muchas variables tenían varianza cercana a cero, esto signiica que son prácticamente constantes, por lo que considerarlas para un modelo es irrelevante ya que no tienen relación alguna con la variable target.
Para esto usamos la funcion *nearZeroVar* de *CARET* que toma dos indicadores para determinar si una variable tiene varianza igual o cercana a cero:

* La frecuencia del valor más común sobre la frecuencia de segundo valor más común (*frequency ratio*). Si la variable se comporta adecuadamente, el ratio deberá ser aproximadamente uno, y será muy grande para casos muy desbalanceados
* El porcentaje de valores únicos: Número de valores únicos entre el total de registros. Debería tender a cero en la medida que se incrementa la granularidad.

```{r}
# Filtramos variables con varianza cero o cercana a cero
dim(data)
nzv <- nearZeroVar(data)
data_filtered <- data[, -nzv]
dim(data_filtered)

```

Volvemos a correr el modelo normalizando las variables que quedan

```{r}
# Normalizo los datos
train_data_norm_filtered <- train_data_norm[,-nzv]
validation_data_norm_filtered <- validation_data_norm[,-nzv]

#preprocess_norm <- preProcess(train_data_filtered, norm = c('center','scale'))
#train_data_norm <- predict(preprocess_norm, train_data_filtered)
#validation_data_norm <- predict(preprocess_norm, validation_data_filtered)

rpartFit_norm_filtered <- train(TARGET ~ .,data = train_data_norm_filtered, 
                                method = 'rpart', 
                                trControl = fitControl,
                                metric = 'ROC')
rpartFit_norm_filtered
```
```{r}
# ROC y AUC
pred_t = predict(rpartFit_norm_filtered, newdata = validation_data_norm_filtered ,type='prob')[,2]
pred2 <- prediction(pred_t, validation_data_norm_filtered$TARGET)
perf <- performance(pred2,"tpr","fpr")
# Graficamos la curva ROC
plot(perf, main="Curva ROC", colorize=T)
```

```{r}
# AUC
pred <- predict(rpartFit_norm_filtered,validation_data_norm_filtered, type = 'prob')
pred$pred <- factor(if_else(pred$churn >= .5, 'churn','no_churn'))
pred$obs <- validation_data_norm_filtered$TARGET

test_results_norm_filtered <- twoClassSummary(pred, lev = levels(pred$obs))
test_results_norm_filtered
```

Vemos que bajo! Evidentemente, como se suele recomendar, no es lo mejor eliminar variables. Puede ocurrir que, incluso con varianza cercana a cero, algunas variables sean importantes para determinar el target. La función nearZeroVar elimina tanto las variables con varianza = 0 como aquellas que tienen varianza cercana a cero. Veamos cuántas tienen varianza cercana a cero y cuántas tienen varianza cero:

```{r}
# Calculo cantidad de variables con cero y casi cero varianza

nzv_desc <- nearZeroVar(data, saveMetrics = TRUE)

columns <- c('Cantidad de variables')
varianza_cero <- nrow(nzv_desc[nzv_desc$zeroVar == TRUE, ])
nz_varianza <- nrow(nzv_desc[nzv_desc$nzv == TRUE, ])

data.frame(columns,varianza_cero, nz_varianza)
```

Como se ve, no hay ninguna variable que tenga varianza cero absoluto, sino que son cercanas a cero. Puede estar ocurriendo que algunas variables que eliminamos correlacionen con la variable target por lo que al eliminarlas, estemos afectando al modelo.

Veamos la importancia de las variables para el modelo normalizado con todas las variables:

```{r,fig.width = 15, fig.height = 10}

# Visualizo las variables relevantes
plot(varImp(rpartFit_norm), top = 30, main = 'Feature Importance')

# Guardo las variables con poca varianza
low_variance_var <- rownames(nzv_desc[nzv_desc$nzv == TRUE,])
# Guardo las variables importantes segun el modelo
important_features <- as.matrix((varImp(rpartFit_log_norm)$importance))

important_features[(rownames(important_features) %in% low_variance_var) & (important_features > 0),]

# Me quedo con las variables importantes que tienen poca varianza

important_low_variance_var <- rownames(as.matrix(important_features[(rownames(important_features) %in% low_variance_var) & (important_features > 0),]))

```

Sorpresa! Algunas de las variables con varianza casi cero son relevantes para el modelo. Esto explica por qué al quitarlas bajó el valor del AUC. No tienen un alto nivel de importancia, pero son relevantes evidentemente.

Por última vez, intentamos correr el modelo con las variables que no tengan varianza cercana a cero, manteniendo aquellas que tengan algún nivel de importancia según el modelo *rpart*. Seguimos usando el modelo normalizado.

```{r}
# Quito las variables con baja varianza que son relevantes del total de varables con baja varianza
nzv_filtered <- setdiff(low_variance_var,important_low_variance_var)
```

```{r}
# Normalizo los datos
#train_data_norm_filtered <- train_data_norm[setdiff(names(train_data_norm),nzv_filtered)]
#validation_data_norm_filtered <- validation_data_norm[setdiff(names(validation_data_norm),nzv_filtered)]

train_data_filtered <- train_data[setdiff(names(train_data), nzv_filtered)]
validation_data_filtered <- validation_data[setdiff(names(validation_data),nzv_filtered)]

preprocess_norm <- preProcess(train_data_filtered, norm = c('center','scale'))
train_data_norm <- predict(preprocess_norm, train_data_filtered)
validation_data_norm <- predict(preprocess_norm, validation_data_filtered)

rpartFit_norm_final <- train(TARGET ~ .,data = train_data_norm, 
                                method = 'rpart', 
                                trControl = fitControl,
                                metric = 'ROC')
rpartFit_norm_final
```
```{r}
# AUC
pred <- predict(rpartFit_norm_final,validation_data_norm, type = 'prob')
pred$pred <- factor(if_else(pred$churn >= .5, 'churn','no_churn'))
pred$obs <- validation_data_norm$TARGET

test_results_norm_filtered <- twoClassSummary(pred, lev = levels(pred$obs))
test_results_norm_filtered
```

Volvió a bajar, evidentemente nuestro criterio para seleccionar las variables en función de la varianza e importancia no fue suficiente como para mantener la performance del modelo.

Por esto decidimos empezar a utilizar algunas herramientas de selección de variables 

* Regresión logística regularizada: Regresión Lasso
* Random Forest
* Correlación entre variables

Esas son algunas de las que existen. La idea no es probar todas sino conseguir que el modelo performe bien y eliminar las variables que realmente no sumen información útil al modelo.

# Regresión logística con regularización lasso

```{r}
# Recordemos que la libreria glmnet consume matrices, no dataframes

train_data_norm$TARGET <- if_else(train_data_norm$TARGET == 'churn',1,0)
validation_data_norm$TARGET <- if_else(validation_data_norm$TARGET == 'churn',1,0)

matrix_train = model.matrix( ~ .-1, train_data_norm)
matrix_test = model.matrix(~.-1, validation_data_norm)

train_col <- setdiff(names(train_data_norm),'TARGET')

x_train <- as.matrix(matrix_train[,-])
y_train = as.matrix(matrix_train[,71]) # 0/1 flag.

x_test <- as.matrix(matrix_test[,-71])
y_test<- as.matrix(matrix_test[,71])



```

