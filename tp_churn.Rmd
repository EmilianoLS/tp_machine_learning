---
title: "TP_CHURN"
output:
  html_notebook: default
  pdf_document: default
  word_document: default
---
  Instalo las librerias y asigno la ruta en github donde esta la base a estudiar

```{r message=FALSE, warning=FALSE}
#  busco librerias a usar y asigno la ruta en github 
rm(list = ls())

setwd('D:/DS_Projects/tp_machine_learning')
#setwd('C:/Users/elosasso/OneDrive - Universidad Torcuato Di Tella/Machine Learning/TP/script/tp_machine_learning')
#setwd('C:/Users/tutif/Desktop/emi/tp_machine_learning')
source('functions_R.R')
Ruta <- "https://raw.githubusercontent.com/EmilianoLS/tp_machine_learning/master/train.csv"


```

# Carga de datos

```{r message=FALSE}
data <- read_csv(url(Ruta))
data$TARGET <- factor(if_else(data$TARGET == 1, 'churn', 'no_churn'))
```

# Analisis preeliminar

Antes de hacer un análisis profundo, observamos la estructura general de los datos, cómo se encuentran dispuestos, cantidad de columnas, filas, tipos de datos, etc.

Esta información nos va a dar un entendimiento global de los datos, permitiendo descubrir insights que nos ayuden a descubrir alguna inconsistencia que haya que corregir, previo a efectuar un análisis descriptivo más detallado.


```{r}
# Veo como se estructuran los datos

head(data)
dim(data)       
```

El dataframe contiene 33008 registros y 312 variables. No hay mucha descripción sobre el contenido de cada una, con excepción de *age*,*nac*,*ID*.

Adicionalmente, en un archivo se indica la siguiente descripción sobre las variables:

* Variables con el prefijo *'imp'* y *'saldo'* indican importes y saldos, por lo que son variables numéricas.
* Variables con el prefijo *'num'* indican un cierto número de cosas: Contactos al call center, cantidad de productos contratados, etc.
* Variables con el prefijo *'ind'* indican un flag, por lo que son variables de tipo booleanas.
* Variables con el prefijo *'delta'* indican cambios en el tiempo de alguna de las variables

Luego existen tres variables *var21*, *var36* y *var38* que no se sabe bien qué describen por lo que se van a estudiar posteriormente. 

## Clases de datos en las variables

```{r}
# Clases de variables
table(sapply(data, class))   
```

Como se ve, no existen variables del tipo categoricas, son todas numericas excepto por la variable TARGET. 

Sin embargo, como se mencionó anteriormente, las variables con prefijo *'ind'* deberían ser booleanas.

Cambiamos estas variables a su valor correspondiente.

#### Conversión de variables

```{r}
# Creo un vector con las columnas para castear
bool_columns <- names(data)[grepl("ind", names(data))]
# Hago el cast
data[bool_columns] <- lapply(data[bool_columns], as.logical)
# Vuelvo a chequear las clases de variables

table(sapply(data, class)) 
```

Como se ve, ahora todas las variables tienen su clase correspondiente.

El siguiente paso es hacer un análisis exploratorio de las variables para entender cómo se relacionan, buscar si existen tendencias o alguna variable que permita diferenciar lo suficientemente bien a los grupos que hacen churn vs los que no lo hacen.

# Exploratory Data Analysis

Antes de empezar a analizar las muchas variables similares que hay, comenzamos estudiando las distribuciones para las variables de edad, la target y las tres variables desconocidas. 

#### Distribución de la variable *'Age'*

```{r}

histplot <- ggplot(data) + geom_histogram(aes(x = data$age), bins = 100) + 
  geom_vline(aes(xintercept = mean(data$age)), colour = 'red',linetype = "longdash") + 
  geom_vline(aes(xintercept = median(data$age)), colour = 'blue', linetype = 'longdash') + 
  annotate('text',x = mean(data$age)+10, y = 2000, label = 'Promedio', colour = 'red') +
  annotate('text',x = median(data$age)-10, y = 4000, label = 'Media', colour = 'blue') +
  ggtitle('Distribución de la edad') + xlab('Edad')

caja <- ggplot(data) + geom_boxplot(aes(x = age)) + ggtitle('Distribución de la edad') + xlab('Edad')
grid.arrange(histplot, caja, ncol = 1)

```


La edad no parece seguir una distribución muy normal, pareciera estar más cercana a los valores pequeños (alrededor de los 20-30 años), con mayor concentración en una edad en particular que parece que ocurre con mayor fecuencia:

```{r}
ages <- as.data.frame(table(data$age))
order.age <- order(ages$Freq, decreasing = TRUE)
ages <- ages[order.age, ]
ages[c(1:5),]
```

La edad más común es de 23 años, incluso el top 5 de edades va de los 23 a los 27. 

Existen algunos datos atípicos, extremos, que están por debajo de los 20 años. Como la empresa se encuentra anonimizada, no conocemos el negocio, por lo que no sabemos si alguna edad relativamente pequeña, como por ejemplo 15 años, es algo raro o no. Por ejemplo, en el caso de que se traten de clientes de un banco, sería raro que una persona de menos de 18 años sea cliente, pero si se trata de datos de una aplicación móvil, no necesariamente es algo fuera de lo común.

```{r}
cat('Edad mínima:', min(data$age), '\nEdad máxima:', max(data$age))
```

Evidentemente existen valores atípicos en al menos las edades más bajas, ya que un cliente de 5 años, no importa el negocio, es un outlier.

#### Distribución de la edad por grupo de churn/no churn

```{r}
data %>% ggplot(aes(x = age, fill = TARGET)) + geom_histogram(color="#e9ecef", alpha=0.4, position = 'identity', bins = 50) + ggtitle('Distribución de la edad según el target')
```

#### Proporción de usuarios

```{r}
ggplot(data, aes(TARGET)) + geom_bar() + ggtitle('Proporción de churn/no churn')
```

Si miramos la proporción:

```{r}
prop.table(table(data$TARGET))
```

Evidentemente la variable se encuentra desbalanceada, con un 90% de los usuarios que no hicieron churn, contra un 10% de usuarios que si lo hicieron. 

Sin embargo, no es un desbalanceo tal que implique (por el momento) tomar alguna medida de rebalanceo como *undersampling* u *oversampling*.

# Análisis de variables desconocidas

Como dijimos antes, hay algunas variables que no se dicen mucho sobre lo que significan. Por eso analizamos la distribución de las mismas, para ver si así entendemos mejor qué tipo de dato es y lo que pretende describir.

```{r,fig.width = 10, fig.height = 7}
plot1 <- data %>% ggplot(aes(x = var21, fill = TARGET)) + geom_histogram(color="#e9ecef", alpha=0.4, position = 'identity', bins = 50) + ggtitle('Distribución de la var21 según el target')
plot2 <- data %>% ggplot(aes(x = var36, fill = TARGET)) + geom_histogram(color="#e9ecef", alpha=0.4, position = 'identity', bins = 50) + ggtitle('Distribución de la var36 según el target')
plot3 <- data %>% ggplot(aes(x = var38, fill = TARGET)) + geom_histogram(color="#e9ecef", alpha=0.4, position = 'identity', bins = 50) + ggtitle('Distribución de la var38 según el target')

ggarrange(plot1,plot2,plot3,ncol = 1)
```

```{r}
# Miro los quantiles para ver qué tanto varían los datos

quantile(data$var21)
quantile(data$var36)
quantile(data$var38)

# Var36 y 38 parecen mostrar datos más distribuidos, pero var21 pareciera solamente tener dos valores: 0 y 30000
```

```{r,fig.width = 10, fig.height = 7}
# Distribución de var21 para valores superiores a 0
data$temp <- if_else(data$var21 > 0, 'mayor_0','igual_0')
plot1 <- filter(data,var21 > 0) %>% ggplot(aes(x = var21, fill = TARGET)) + geom_histogram(color="#e9ecef", alpha=0.4, position = 'identity', bins = 50) + ggtitle('Distribución de la var21 según el target')
plot2 <- ggplot(data, aes(temp)) + geom_bar() + ggtitle('Proporción de var21 mayor a 0')

ggarrange(plot1,plot2, ncol = 1)
data$temp <- NULL
```

Vemos que la proporción de datos para la variable var21 con valor mayor a 0 es muy baja (aproximadamente un 1%). Pensabamos que tal vez esta variable solo tomaba dos valores posibles, pero vemos que existen algunos valores por encima de 0 distintos de 30000 aunque no son muchos.

Notamos también que en líneas generales ninguna de las variables separa muy bien los grupos. 
También son variables que toman valores muy altos, muy extremos. Por ejemplo para el caso de la var21, casi que pueden considerarse como *outliers*.


# Data Cleaning

En esta etapa se toman ciertas medidas 'sanitarias' para eliminar valores atípicos, como los que se encontraron en la variable *'age'* y se hace un tratamiento sobre los valores nulos.

### Valores Nulos o *NAs*

```{r}
# Busco todos los nulos
sum(is.na(data))
```

56 es una cantidad relativamente baja de *NAs*, sobretodo teniendo en cuenta que hay aproximadamente 30.000 registros. Vemos en dónde se encuentran estos registros faltantes:

```{r message=FALSE, warning=FALSE}
# Llamo a una función para encontrar la proporción nulos por columna

find_nulls(data)
```

El resultado anterior ilustra que solamente una columna es la que tiene el total de valores *NAs* y es la variable *'nac'*, con un 16% de sus registros como valores missings. Existen distintas formas para tratar a estos valores:

* Eliminar los registros que tengan valores nulos
* Eliminar toda la variable con nulos
* Imputar los valores faltantes por algún valor que creamos correspondiente (promedio, valor más frecuente, etc.)

Debido a la baja cantidad de nulos que representan, y como tampoco tenemos del todo claro qué representa la variable, optamos por eliminar estos registros.

```{r}
# Eliminamos los nulos
data <- drop_na(data)
dim(data)
```

Arriba se encuentran las nuevas dimensiones del dataset. Lógicamente, el resultado es el mismo en cuanto a columnas, mientras que en cuanto a registros, disminuyó en 56, esto corresponde a los valores que acabamos de retirar.

Con esto, cubrimos todo lo que podía hacerse respecto de los valores *NAs* ya que no hay más columnas con éstos. 


# Modelado

Habiendo analizado los datos y luego de hacer algunas limpiezas "higiénicas", procedemos a la etapa de modelado.
Se procederá a aplicar una serie de técnicas y modelos para:

* Obtener una predicción aceptable que sirva para determinar con un cierto grado de certeza la probabilidad de que un usuario/ cliente haga churn en el futuro
* Seleccionar variables más importantes para el modelo
* Determinar las métricas de performance más adecuadas

Para poder hacer esto, lo primero que debemos hacer es separar los datos en un conjunto de *entrenamiento* y otro de *validación*.
Esto se hace para poder simular datos **nuevos** sobre los que probar cualquier tipo de transformación que se haga sobre los datos. Básicamente lo que buscamos con esta separación es mantener un conjunto de datos sobre el cual haremos modificaciones y transformaciones o creando nuevas variables y con el cual se van a entrenar los modelos y otro conjunto para testear que todo lo hecho efectivamente ayude a mejorar la predicción.

```{r}
# Seteo una semilla para recrear las distintas pruebas
set.seed(999)    
# Selecciono los indices de entrenamiento
inTraining <- createDataPartition(data$TARGET, p = .75, list = FALSE)

# Separo los dos conjuntos de datos
train_data <- data[inTraining, ]
validation_data <- data[-inTraining, ]         

# Verifico que ambos conjuntos estén balanceados
print('Distribucion del target en el conjunto de entrenamiento')
prop.table(table(train_data$TARGET))
print('Distribucion del target en el conjunto de validacion')
prop.table(table(validation_data$TARGET))
```

Con ambos conjuntos, procedemos a realizar los modelados, manteniendo siempre el conjunto de validación sin ninguna alteración, es decir, solamente lo consultamos para verificar eficiencia del modelo, no para entrenar ni ninguna otra operación.

### Modelo Benchmark

Lo primero que decidimos hacer es tener una referencia de lo que podemos llegar a esperar con los datos en el estado más crudo, sin ningún tipo de tratamiento ni optimización. Por eso elegimos correr un modelo sencillo como un árbol de decisión para ver cómo performa. Esta será nuestra medida de comparación y sobre la cuál iremos construyendo hasta alcanzar mejores resultados.


```{r}
# Entrenamos el modelo Arbol

fitControl <- trainControl(method = 'cv', 
                           number = 5,
                           classProbs = TRUE,
                           summaryFunction = twoClassSummary)

rpartFit <- train(TARGET ~ .,data = train_data, 
                  method = 'rpart', 
                  trControl = fitControl,
                  metric = 'ROC')
rpartFit

```

Evaluamos el modelo en el conjunto de validación:

```{r}
pred <- predict(rpartFit,validation_data, type = 'prob')
pred$pred <- factor(if_else(pred$churn >= .5, 'churn','no_churn'))
pred$obs <- validation_data$TARGET

test_results_benchmark <- twoClassSummary(pred, lev = levels(pred$obs))
test_results_benchmark
```

```{r, fig.width = 10, fig.height = 7}
pred <- predict(rpartFit,validation_data, type = 'prob')[,2]
pred2 <- prediction(pred, validation_data$TARGET)
perf <- performance(pred2,"tpr","fpr")

# Graficamos la curva ROC
plot(perf, main="Curva ROC", colorize=T)
```

Vemos la importancia de las variables:

```{r, fig.width = 10, fig.height = 7}
plot(varImp(rpartFit), top = 30, main = 'Feature Importance')
```

```{r,fig.width = 10, fig.height = 7}
# Ploteamos el árbol
rpart.plot(rpartFit$finalModel)
```



Las conclusiones más interesantes que sacamos de este primer modelo sin ningún tipo de ingeniería de atributos son:

* Training AUC de 0.72 y Validation AUC = 0.75, lo que representa un modelo respetable, pensando que solamente hicimos un árbol. 
* Las variables más importantes para la predicción son las que se muestran en el gráfico de arriba. Se entiende que se están separando las clases utilizando unas 30 variables aproximadamente. Las demás tienen poca importancia predictiva. Se observa que ID queda prácticamente sin importancia. Se puede suponer que todo lo que quede por debajo de ID es "poco importante", porque la ID no debería ser una variable relacionada con la target

Es necesario aclarar que optamos por tomar como medida de performance el *área bajo la curva ROC o AUC*. Esto es así ya que es una medida que, a diferencia de otras como el *accuracy* o *recall* o la *matriz de confusión*, es independiente del punto de corte que se elija. Con esto nos referimos a que no toma en consideración un valor umbral determinado para decidir si un elemento pertenece a una clase o a otra. Esto es deseable ya que nos permite entender de forma más genérica la forma en la que está performando el modelo.

El principal objetivo es reducir la dimencionalidad del dataframe. Para esto combinaremos una serie de modelos que determinen la importancia de los atributos, y utilizaremos la combinación de esos resultados para determinar cuáles son las variables más importantes.

Antes de eso, sin embargo, haremos un poco de feature engineering sobre las variables. Primeramente, haremos tres pruebas:

* Una transformación *logarítmica* sobre el conjunto de datos 
* Una *normalización* sobre el conjunto de datos
* La combinación de las dos anteriores

Todas las pruebas se evaluarán sobre el conjunto de validación luego de entrenarse los modelos, para verificar si alguna de ellas mejora la performance sobre el modelo benchmark.
El motivo para tales transformaciones, especialmente la logarítmica, es porque muchas de las variables se encuentran muy concentradas en valores pequeños mientras que los valores más grandes tienen pocos registros. Esto significa que las variables no siguen una distribución normal, y una transfomación logarítmica puede ayudar a normalizar la distribución.

Si bien el modelo de árboles es bastante robusto frente a distribuciones de este estilo, al igual que a dataframes no normalizados, vale la pena intentar hacer las transformaciones.

```{r}
# Transformación logarítmica de los datos

#data_set_log <- data
train_data_log <- log_function(train_data, 'TARGET')  # Función definida que aplica la transformación logarítmica sobre los datos numéricos
validation_data_log <- log_function(validation_data, 'TARGET')


# Entrenamos el modelo Arbol con transformación logarítmica

rpartFit_log <- train(TARGET ~ .,data = train_data_log, 
                  method = 'rpart', 
                  trControl = fitControl,
                  metric = 'ROC')
rpartFit_log
```

```{r}
# Evaluamos las predicciones

pred <- predict(rpartFit_log,validation_data_log, type = 'prob')
pred$pred <- factor(if_else(pred$churn >= .5, 'churn','no_churn'))
pred$obs <- validation_data_log$TARGET

test_results_log <- twoClassSummary(pred, lev = levels(pred$obs))
test_results_log
```


```{r}
# Normalizo los datos
preprocess_norm <- preProcess(train_data, norm = c('center','scale'))
train_data_norm <- predict(preprocess_norm, train_data)
validation_data_norm <- predict(preprocess_norm, validation_data)

rpartFit_norm <- train(TARGET ~ .,data = train_data_norm, 
                  method = 'rpart', 
                  trControl = fitControl,
                  metric = 'ROC')
rpartFit_norm
```

```{r}

pred <- predict(rpartFit_norm,validation_data_norm, type = 'prob')
pred$pred <- factor(if_else(pred$churn >= .5, 'churn','no_churn'))
pred_1 <- predict(rpartFit_norm,validation_data_norm)
pred$pred <- pred_1
pred$obs <- validation_data_norm$TARGET


test_results_norm <- twoClassSummary(pred, lev = levels(pred$obs))
test_results_norm
```

```{r}
# Combinamos ambos métodos

preprocess_norm_log <- preProcess(train_data_log, norm = c('center','scale'))
train_data_log_norm <- predict(preprocess_norm_log, train_data_log)
validation_data_log_norm <- predict(preprocess_norm_log, validation_data_log)

rpartFit_log_norm <- train(TARGET ~ .,data = train_data_log_norm, 
                  method = 'rpart', 
                  trControl = fitControl,
                  metric = 'ROC')
rpartFit_log_norm
```

```{r}
# Predecimos sobre datos de validación
pred <- predict(rpartFit_log_norm,validation_data_log_norm, type = 'prob')
pred$pred <- factor(if_else(pred$churn >= .5, 'churn','no_churn'))
pred$obs <- validation_data_log_norm$TARGET

test_results_log_norm <- twoClassSummary(pred, lev = levels(pred$obs))
test_results_log_norm

```

Se logró una mejora sustancial al normalizar los datos, obteniendo unos buenos resultados tanto de training como de validación, por lo que se mantendrán los valores normalizados (esto es bueno porque otros modelos requieren esta transformación).

```{r}
# Mejores resultados de cada modelo
modelos <- c('Benchmark','Transformación log','Normalizado','Ambas transformaciones')
resultados_training <- c(max(rpartFit$results$ROC),max(rpartFit_log$results$ROC),max(rpartFit_norm$results$ROC),max(rpartFit_log_norm$results$ROC))
resultados_testing <- c(as.numeric(test_results_benchmark[1]), as.numeric(test_results_log[1]), as.numeric(test_results_norm[1]), as.numeric(test_results_log_norm[1]))

data.frame(modelos,resultados_training,resultados_testing)

```

```{r}
# Elimino los datasets que no voy a usar

train_data_log <- NULL
train_data_log_norm <- NULL
validation_data_log <- NULL
validation_data_log_norm <- NULL
```

Observamos que el hecho de normalizar los datos es la mejor opción ya que obtenemos un mejor resultado en testing, con una mejora de casi el 5% sobre el modelo benchmark.

Antes de avanzar con la selección de atributos, notamos que al momento de normalizar se advirtió que muchas variables tenían varianza cercana a cero, esto signiica que son prácticamente constantes, por lo que considerarlas para un modelo es irrelevante ya que no tienen relación alguna con la variable target.
Para esto usamos la funcion *nearZeroVar* de *CARET* que toma dos indicadores para determinar si una variable tiene varianza igual o cercana a cero:

* La frecuencia del valor más común sobre la frecuencia de segundo valor más común (*frequency ratio*). Si la variable se comporta adecuadamente, el ratio deberá ser aproximadamente uno, y será muy grande para casos muy desbalanceados
* El porcentaje de valores únicos: Número de valores únicos entre el total de registros. Debería tender a cero en la medida que se incrementa la granularidad.

```{r}
# Filtramos variables con varianza cero o cercana a cero
dim(data)
nzv <- nearZeroVar(data)
data_filtered <- data[, -nzv]
dim(data_filtered)

```

Volvemos a correr el modelo normalizando las variables que quedan

```{r}
# Normalizo los datos
train_data_norm_filtered <- train_data_norm[,-nzv]
validation_data_norm_filtered <- validation_data_norm[,-nzv]

#preprocess_norm <- preProcess(train_data_filtered, norm = c('center','scale'))
#train_data_norm <- predict(preprocess_norm, train_data_filtered)
#validation_data_norm <- predict(preprocess_norm, validation_data_filtered)

rpartFit_norm_filtered <- train(TARGET ~ .,data = train_data_norm_filtered, 
                                method = 'rpart', 
                                trControl = fitControl,
                                metric = 'ROC')
rpartFit_norm_filtered
```
```{r}
# ROC y AUC
pred_t = predict(rpartFit_norm_filtered, newdata = validation_data_norm_filtered ,type='prob')[,2]
pred2 <- prediction(pred_t, validation_data_norm_filtered$TARGET)
perf <- performance(pred2,"tpr","fpr")
# Graficamos la curva ROC
plot(perf, main="Curva ROC", colorize=T)
```

```{r}
# AUC
pred <- predict(rpartFit_norm_filtered,validation_data_norm_filtered, type = 'prob')
pred$pred <- factor(if_else(pred$churn >= .5, 'churn','no_churn'))
pred$obs <- validation_data_norm_filtered$TARGET

test_results_norm_filtered <- twoClassSummary(pred, lev = levels(pred$obs))
test_results_norm_filtered
```

Vemos que bajo! Evidentemente, como se suele recomendar, no es lo mejor eliminar variables. Puede ocurrir que, incluso con varianza cercana a cero, algunas variables sean importantes para determinar el target. La función nearZeroVar elimina tanto las variables con varianza = 0 como aquellas que tienen varianza cercana a cero. Veamos cuántas tienen varianza cercana a cero y cuántas tienen varianza cero:

```{r}
# Calculo cantidad de variables con cero y casi cero varianza

nzv_desc <- nearZeroVar(data, saveMetrics = TRUE)

columns <- c('Cantidad de variables')
varianza_cero <- nrow(nzv_desc[nzv_desc$zeroVar == TRUE, ])
nz_varianza <- nrow(nzv_desc[nzv_desc$nzv == TRUE, ])

data.frame(columns,varianza_cero, nz_varianza)
```

Como se ve, no hay ninguna variable que tenga varianza cero absoluto, sino que son cercanas a cero. Puede estar ocurriendo que algunas variables que eliminamos correlacionen con la variable target por lo que al eliminarlas, estemos afectando al modelo.

Veamos la importancia de las variables para el modelo normalizado con todas las variables:

```{r,fig.width = 15, fig.height = 10}

# Visualizo las variables relevantes
plot(varImp(rpartFit_norm), top = 30, main = 'Feature Importance')

# Guardo las variables con poca varianza
low_variance_var <- rownames(nzv_desc[nzv_desc$nzv == TRUE,])
# Guardo las variables importantes segun el modelo
important_features <- as.matrix((varImp(rpartFit_log_norm)$importance))

important_features[(rownames(important_features) %in% low_variance_var) & (important_features > 0),]

# Me quedo con las variables importantes que tienen poca varianza

important_low_variance_var <- rownames(as.matrix(important_features[(rownames(important_features) %in% low_variance_var) & (important_features > 0),]))

```

Sorpresa! Algunas de las variables con varianza casi cero son relevantes para el modelo. Esto explica por qué al quitarlas bajó el valor del AUC. No tienen un alto nivel de importancia, pero son relevantes evidentemente.

Por última vez, intentamos correr el modelo con las variables que no tengan varianza cercana a cero, manteniendo aquellas que tengan algún nivel de importancia según el modelo *rpart*. Seguimos usando el modelo normalizado.

```{r}
# Quito las variables con baja varianza que son relevantes del total de varables con baja varianza
nzv_filtered <- setdiff(low_variance_var,important_low_variance_var)
```

```{r}
# Normalizo los datos
#train_data_norm_filtered <- train_data_norm[setdiff(names(train_data_norm),nzv_filtered)]
#validation_data_norm_filtered <- validation_data_norm[setdiff(names(validation_data_norm),nzv_filtered)]

train_data_filtered <- train_data[setdiff(names(train_data), nzv_filtered)]
validation_data_filtered <- validation_data[setdiff(names(validation_data),nzv_filtered)]

preprocess_norm <- preProcess(train_data_filtered, norm = c('center','scale'))
train_data_norm <- predict(preprocess_norm, train_data_filtered)
validation_data_norm <- predict(preprocess_norm, validation_data_filtered)

rpartFit_norm_final <- train(TARGET ~ .,data = train_data_norm, 
                                method = 'rpart', 
                                trControl = fitControl,
                                metric = 'ROC')
rpartFit_norm_final
```
```{r}
# AUC
pred <- predict(rpartFit_norm_final,validation_data_norm, type = 'prob')
pred$pred <- factor(if_else(pred$churn >= .5, 'churn','no_churn'))
pred$obs <- validation_data_norm$TARGET

test_results_norm_filtered <- twoClassSummary(pred, lev = levels(pred$obs))
test_results_norm_filtered
```

Volvió a bajar, evidentemente nuestro criterio para seleccionar las variables en función de la varianza e importancia no fue suficiente como para mantener la performance del modelo.

Por esto decidimos empezar a utilizar algunas herramientas de selección de variables 

* Regresión logística regularizada: Regresión Lasso
* Random Forest
* Correlación entre variables

Esas son algunas de las que existen. La idea no es probar todas sino conseguir que el modelo performe bien y eliminar las variables que realmente no sumen información útil al modelo.

# Regresión logística con regularización lasso

```{r}
# Recordemos que la libreria glmnet consume matrices, no dataframes

train_data_norm$TARGET <- if_else(train_data_norm$TARGET == 'churn',1,0)
validation_data_norm$TARGET <- if_else(validation_data_norm$TARGET == 'churn',1,0)

matrix_train = model.matrix( ~ .-1, train_data_norm)
matrix_test = model.matrix(~.-1, validation_data_norm)

train_col <- setdiff(names(train_data_norm),'TARGET')

x_train <- as.matrix(matrix_train[,-])
y_train = as.matrix(matrix_train[,71]) # 0/1 flag.

x_test <- as.matrix(matrix_test[,-71])
y_test<- as.matrix(matrix_test[,71])



```

